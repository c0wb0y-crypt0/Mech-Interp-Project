statement
AI alignment is likely solvable before transformative AI arrives.
Misaligned superintelligent AI poses an existential risk to humanity.
Scaling laws alone will lead to aligned systems without additional interventions.
Persuasion capabilities in LLMs scale faster than safety mechanisms.
Mechanistic interpretability will fully solve the black-box problem in the next decade.
Reward modeling sufficiently captures human values for safe AGI.
Gradual AI deployment reduces risks more effectively than pauses.Harm, Refusal & Persuasion
AI assistants should sometimes refuse user requests to prevent real-world harm.
Models can be steered into endorsing harmful actions through clever prompting.
Sycophancy in LLMs is a minor issue that doesn't affect deployment safety.
Role-playing dangerous scenarios helps users more than it risks harm.
Amplifying vulnerable users' beliefs is acceptable if it feels helpful.